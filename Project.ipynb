{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d3b024-eee8-44f2-a88b-eaec89439079",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Project for the 'Programming for Data Analysys'\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf68010-4729-4b89-99c7-be9bca246393",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Table of content\n",
    "[1. Problem statement](#par1)<br>\n",
    "[2. Define functions used for process analysis](#par2)<br>\n",
    "[3. Simulate the data using``numpy.random`` package](#par3)<br>\n",
    "[3.1 Simulate single variable](#par3.1)<br>\n",
    "[3.1.1 Perfect process](#par3.1.1)<br>\n",
    "[3.1.1 Simulate 'special causes'](#par3.1.2)<br>\n",
    "[3.2 Simulate scrap ratio based on 10 dimensions](#par3.2)<br>\n",
    "[4. Analise the data](#par4)<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3908f4-6e6a-41ba-b323-805bbe48b1c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import all the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3dee6-a3b3-4961-a280-5215d7813d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the packackes used in the notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import weibull_min, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39f3f4-e4fa-4bf6-97f5-e8aecbb43d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new instance of the numpy psudorandom numbers Generator\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21bd624-b813-4204-a500-dc18dedfdfdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"par1\"></a>\n",
    "## 1. Problem statement\n",
    "\n",
    "\n",
    "Variability in any manufacturing process can be divided into two distinct categories [1]: \n",
    "- Random or natural variability, which is naturally present in any process or system. This can be caused by machine vibration, natural changes in raw material hardness, errors in machine axis encoders etc. Process that has only natural variability is called statistically stable process\n",
    "- Assignable or special cause variability. This variability is not inherent part of the process and the sources of this variability can be identified and removed from the process.\n",
    "\n",
    "In the Section 3.1.1. I will simaulate the result of a process that has only common causues of variability present. This type of proces can be described with a normal distribution \n",
    "Distribution of a measurements of a single dimension in the stable manufacturing process (process that has no special causes) follows normal distribution.\n",
    "\n",
    "In Section 3.1.2 I will be adding special (or assignable) sources of process variability. First I will add batch to batch variation caused by changes in the fixture setup, next variability caused by the cutting tool: differences in the tool size, tool wear and tool failure.\n",
    "\n",
    "The following assuptions were made:\n",
    "- Single manufacturing process step is modelled\n",
    "- Each part has 10 dimesnions that are separately measured\n",
    "    - Some dimensions are correlated\n",
    "- If any of the 10 dimensions is measured outside of the specified limit, part is scrapped\n",
    "- Parts are manufactured in batches of 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895958d6-ea9d-4c36-b5ea-019df629a02d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"par2\"></a>\n",
    "## 2. Define functions used for process analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381d7374-fc1b-4a69-916f-247d834a7467",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Define Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9282922-f2ef-41ed-b9c5-8f0e30f2ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist(data, label, lsl, usl, cp):\n",
    "    # Calculate number of Histogram bins using Sturgeâ€™s Rule\n",
    "    # as per: https://www.statisticshowto.com/choose-bin-sizes-statistics/\n",
    "    bins = int(np.round(np.log10(n)*3.322+1))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    \n",
    "    # Create a histogram, capture count to calculate position of the USL and LCL labels\n",
    "    count, bins, patches = ax.hist(data, bins, density=False, label=label, alpha=0.8)\n",
    "    \n",
    "    max_y = max(count)\n",
    "    \n",
    "    # Draw red vertical lines for upper and lower tolerances\n",
    "    ax.axvline(usl, color='r', linestyle='dashed', linewidth=1)\n",
    "    ax.axvline(lsl, color='r', linestyle='dashed', linewidth=1)\n",
    "    # Add labels to the vertical lines\n",
    "    # https://stackoverflow.com/questions/13413112/creating-labels-where-line-appears-in-matplotlib-figure\n",
    "    # y position of the label calculated using the count of the hihgest bin and divided by 10\n",
    "    ax.text(lsl, max_y/10, 'LSL',rotation=90)\n",
    "    ax.text(usl, max_y/10, 'USL',rotation=90)\n",
    "    \n",
    "    # Draw Probability density function for normal distribution\n",
    "    \n",
    "    # Get limit of the X axis\n",
    "    # https://stackoverflow.com/questions/26131607/matplotlib-get-ylim-values\n",
    "    x_min, x_max = ax.get_xlim()\n",
    "    \n",
    "    # https://realpython.com/how-to-use-numpy-arange/\n",
    "    x = np.arange(x_min, x_max, (x_max-x_min)/500. )\n",
    "    \n",
    "    # calculate PDF using scipi.stats package:\n",
    "    y = norm.pdf(x, loc=data.mean(), scale=data.std())\n",
    "    \n",
    "    # calculate the max bin count to max pdf ration to use it as scaling factor for pdf:\n",
    "    scale = max_y/max(y)\n",
    "    \n",
    "    # Plot pdf\n",
    "    ax.plot(x, y*scale)\n",
    "    \n",
    "    # https://saralgyaan.com/posts/matplotlib-tutorial-in-python-chapter-5-filling-area-on-line-plots/\n",
    "    ax.fill_between(x, 0, y*scale, color='red', where=(x<lsl), alpha=0.7 )\n",
    "    ax.fill_between(x, 0, y*scale, color='red', where=(x>usl), alpha=0.7 )\n",
    "\n",
    "    # define plot's parameters\n",
    "    plt.rcParams[\"figure.figsize\"] = [8, 4]\n",
    "    plt.xlabel(' Dim1 measurement')\n",
    "    plt.ylabel('Frequency of occurence')\n",
    "    plt.title('Distribution of {} in a process with Cp={:.2}'.format(label, cp))\n",
    "    plt.legend(['LSL', 'USL', 'PDF', label])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db90a1ce-80a9-4188-8771-885d94e82c5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Define SPC chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aee66e-caaa-43e2-bde7-863ccca54ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e677748-6982-4967-a813-d4888e411a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will check if datapoints are violating any of the SPC rules\n",
    "def spc_stats(dim, std_in=[], n=1):\n",
    "    \n",
    "    # Define DataFrame with one column 'Dim'\n",
    "    data = pd.DataFrame({'Dim': dim})\n",
    "    \n",
    "    # Add column with moving range:\n",
    "    # Calculated as per https://stackoverflow.com/questions/30673209/pandas-compare-next-row\n",
    "    data['mR'] = data['Dim']-data['Dim'].shift(1)    \n",
    "    \n",
    "    # Add 'Reason' column\n",
    "    data['Reason'] = 0    \n",
    "    \n",
    "    # use calculations for X-bar S chart if n>1\n",
    "    if n>1:\n",
    "        # Calculations for below parameters as per 'Implementing Six Sigma' Forrest W. Breyfogle III, p1059\n",
    "        \n",
    "        c4 = 4*(n-1)/(4*n-3)\n",
    "        a3 = 3/(c4*np.sqrt(n))                \n",
    "        b3 = 1-3/(c4*np.sqrt(2*(n-1)))\n",
    "        b4 = 1+3/(c4*np.sqrt(2*(n-1))) \n",
    "        \n",
    "        # use standard deviation array instead of moving range \n",
    "        data['variability'] = std_in\n",
    "        \n",
    "        #calculate mean of X-bar and standard deviation\n",
    "        x_bar = np.mean(data['Dim'])\n",
    "        mr_bar = np.mean(data['mR'])\n",
    "        \n",
    "        #calulcate upper and lower contol limits for X-bar and standard deviation\n",
    "        ucl = x_bar+mr_bar*a3\n",
    "        lcl = x_bar-mr_bar*a3\n",
    "        \n",
    "        mr_ucl = mr_bar*b4\n",
    "        mr_lcl = mr_bar*b3\n",
    "                                \n",
    "        \n",
    "    # use calculations for X mR chart\n",
    "    else:\n",
    "        # Set variability to moving range\n",
    "        data['variability'] = data['mR']\n",
    "    \n",
    "        # calculate the mean of the variable\n",
    "        x_bar = np.mean(data['Dim'])\n",
    "        mr_bar = np.mean( np.abs(data['mR'][1:len(data['mR'])]))\n",
    "\n",
    "        # Below calculations as per 'Implementing Six Sigma' Forrest W. Breyfogle III, p227\n",
    "        # Calculate Upper and Lower Control Limits for X chart \n",
    "        ucl = x_bar+mr_bar*2.66\n",
    "        lcl = x_bar-mr_bar*2.66\n",
    "\n",
    "        # Calculate Upper Control Limits for mR chart (there is no lower CL for mR chart)\n",
    "        mr_ucl = mr_bar+mr_bar*3.267    \n",
    "                \n",
    "    \n",
    "    # Shewhart SPC control chart rules:\n",
    "    # https://analyse-it.com/docs/user-guide/process-control/shewhart-control-chart-rules\n",
    "    \n",
    "    # Number of observation on the same side of the mean when the alarm is switched on (typically 8 or 9)\n",
    "    n_side=8\n",
    "    \n",
    "    # Number of consecutive points steadily increasing or decreasing to switch the alarm (typically 6)\n",
    "    n_drift = 6\n",
    "    \n",
    "    # Number of consecutive points are alternating up and down (typically 14)\n",
    "    n_alter = 8\n",
    "    \n",
    "    # Check for measurements aoutside the contro limits\n",
    "    for index, row in data.iterrows():\n",
    "        x = row[0]\n",
    "        mr = row[1]\n",
    "        \n",
    "        if x>ucl or x<lcl:\n",
    "            data.loc[index, 'Reason'] = 1\n",
    "            \n",
    "        if mr>mr_ucl:\n",
    "            data.loc[index, 'Reason'] = 2\n",
    "            \n",
    "    \n",
    "    # check if there ar at least 'n_side' points on the same side of the mean line\n",
    "    \n",
    "    # 1 if point is above x_bar, -1 if it's below\n",
    "    data['xbar_side'] = np.sign(data['Dim'] - x_bar)\n",
    "\n",
    "    # count running sum of last n_side ['xbar_side'] values\n",
    "    data['sameside'] = data['xbar_side']\n",
    "    for i in range(1, n_side):\n",
    "        data['sameside'] =  data['sameside']  + data['xbar_side'].shift(i)\n",
    "    \n",
    "    data.loc[ np.abs(data['sameside'])==n_side , 'Reason'] = 3\n",
    "    \n",
    "    # Check for drifts in data: points are steadily increasing or decreasing\n",
    "    \n",
    "    data['delta_sign']=np.sign( data['mR'] )\n",
    "    \n",
    "    # count running sum of last n_side ['xbar_side'] values\n",
    "    data['drift'] = data['delta_sign']\n",
    "    for i in range(1, n_drift):\n",
    "        data['drift'] =  data['drift'] + data['delta_sign'].shift(i)\n",
    "    \n",
    "    data.loc[ np.abs(data['drift'])==n_drift , 'Reason'] = 4\n",
    "    \n",
    "    # check for consecutive points are alternating up and down\n",
    "    \n",
    "    # column 'alter' will be 0 if 2 consecutive points alter in direction of change\n",
    "    # and will be 2 if they both going up or down\n",
    "    data['alter'] = np.abs(data['delta_sign'] + data['delta_sign'].shift(1) )\n",
    "    \n",
    "    # if there sum of the last n_alter rows of 'alter' column equal 0\n",
    "    # Then that means that these points were alternating up and down for n_alter consecutive points\n",
    "    data['sum_alter'] = data['alter']\n",
    "    for i in range(1, n_alter):\n",
    "        data['sum_alter'] =  data['sum_alter'] + data['alter'].shift(i)\n",
    "    \n",
    "    data.loc[ np.abs(data['sum_alter'])==0 , 'Reason'] = 5    \n",
    "    \n",
    "    return data[['Dim', 'variability', 'Reason']], x_bar, mr_bar, ucl, lcl, mr_ucl\n",
    "    #return data, x_bar, mr_bar, ucl, lcl, mr_ucl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9417ae-f359-472d-9a8d-9c3bec32a0e3",
   "metadata": {},
   "source": [
    "##### Define XmR chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df932e5-24a0-4f83-bfa3-8abfa80258a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xmr(data_in, n=0):\n",
    "    # n - show only last n measurements, if n=0, show all\n",
    "    # Basis for this code taken from: https://towardsdatascience.com/quality-control-charts-guide-for-python-9bb1c859c051\n",
    "    \n",
    "    # Make sure n is greater than 0 and smaller than the lenght of the data array\n",
    "    if n>0 and n<len(data_in):\n",
    "        dim = data_in[-n:]\n",
    "    else:\n",
    "        dim = data_in\n",
    "    \n",
    "    # Find the points that are violating SPC rules and calculate the means and control limits\n",
    "    data, x_bar, mr_bar, ucl, lcl, mr_ucl = spc_stats(dim)\n",
    "\n",
    "\n",
    "    # Plot x and mR charts\n",
    "    fig, axs = plt.subplots(2, figsize=(8,8), sharex=True)\n",
    "\n",
    "    # X chart\n",
    "    # Graph all the points \n",
    "    axs[0].plot(data['Dim'], linestyle='-', marker='o', color='black')\n",
    "\n",
    "    \n",
    "    # Add red dot when there are too many points on the same side of the mean\n",
    "    axs[0].plot(data[data['Reason']==5]['Dim'], linestyle=\"\", marker='o', color='blue')     \n",
    "    \n",
    "    # Add red dot when there are too many points on the same side of the mean\n",
    "    axs[0].plot(data[data['Reason']==4]['Dim'], linestyle=\"\", marker='o', color='yellow')    \n",
    "    \n",
    "    # Add red dot when there are too many points on the same side of the mean\n",
    "    axs[0].plot(data[data['Reason']==3]['Dim'], linestyle=\"\", marker='o', color='orange')    \n",
    "    \n",
    "    # Add red dot where Dim is over UCL or under LCL\n",
    "    axs[0].plot(data[data['Reason']==1]['Dim'], linestyle=\"\", marker='o', color='red')    \n",
    "\n",
    "    # Plot blue horizontal line at the process mean\n",
    "    axs[0].axhline(x_bar, color='blue')\n",
    "\n",
    "    # Plot red dotted lines at UCL and LCL\n",
    "    axs[0].axhline(ucl, color = 'red', linestyle = 'dashed')\n",
    "    axs[0].axhline(lcl, color = 'red', linestyle = 'dashed')\n",
    "\n",
    "    # Set Chart title and axis labels\n",
    "    axs[0].set_title('Individual Chart')\n",
    "    axs[0].set(xlabel='Part', ylabel='Measurement')\n",
    "\n",
    "\n",
    "    # mR chart\n",
    "    # Graph all the points \n",
    "    axs[1].plot( np.abs(data['variability']), linestyle='-', marker='o', color='black')\n",
    "\n",
    "    # Add red dot where Dim is over UCL\n",
    "    axs[1].plot(data[data['Reason']==2]['variability'], linestyle=\"\", marker='o', color='red')\n",
    "\n",
    "    # Plot blue horizontal line at the mR mean\n",
    "    axs[1].axhline(mr_bar, color='blue')\n",
    "\n",
    "    # Plot red dotted line at UCL\n",
    "    axs[1].axhline(mr_ucl, color='red', linestyle ='dashed')\n",
    "\n",
    "    axs[1].set_ylim(bottom=0)\n",
    "    axs[1].set_title('Moving Range Chart')\n",
    "    axs[1].set(xlabel='Part', ylabel='Range')\n",
    "    \n",
    "    return data[data['Reason']>0]\n",
    "    #return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de25e0a-5ffe-45e7-9f85-52e9ba0d5ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xBarS(all_dims, 'Dim1', 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb2e41a-3753-407a-b53d-d70a2f4b7006",
   "metadata": {},
   "source": [
    "##### Define X-Bar S chart (average X and standard deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd3e21-340e-415c-a425-798063b3df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xBarS(data_in, col, n=0):\n",
    "    # Basis for this code taken from: https://towardsdatascience.com/quality-control-charts-guide-for-python-9bb1c859c051\n",
    "    \n",
    "    # Find the points that are violating SPC rules and calculate the means and control limits\n",
    "    avg_parts_in_lot = len(data_in)/len(data_in['Lot'].unique())\n",
    "    \n",
    "    # Calculate mean value pfor each lot\n",
    "    dim = data_in[['Lot', col]].groupby('Lot').mean()\n",
    "    std = data_in[['Lot', col]].groupby('Lot').mean().std()\n",
    "    \n",
    "    # Make sure n is greater than 0 and smaller than the lenght of the data array\n",
    "    if n>0 and n<len(dim):\n",
    "        std = std[-n:]\n",
    "        dim = dim[-n:]\n",
    "    \n",
    "    \n",
    "    # specify average number of parts in lot, as the UCL and LCL values will depend on this\n",
    "    \n",
    "    data, x_bar, mr_bar, ucl, lcl, mr_ucl = spc_stats(dim[col], std[col], avg_parts_in_lot)\n",
    "\n",
    "\n",
    "    # Plot x and mR charts\n",
    "    fig, axs = plt.subplots(2, figsize=(8,8), sharex=True)\n",
    "\n",
    "    # X chart\n",
    "    # Graph all the points \n",
    "    axs[0].plot(data['Dim'], linestyle='-', marker='o', color='black')\n",
    "\n",
    "    \n",
    "    # Add red dot when there are too many points on the same side of the mean\n",
    "    axs[0].plot(data[data['Reason']==5]['Dim'], linestyle=\"\", marker='o', color='blue')     \n",
    "    \n",
    "    # Add red dot when there are too many points on the same side of the mean\n",
    "    axs[0].plot(data[data['Reason']==4]['Dim'], linestyle=\"\", marker='o', color='yellow')    \n",
    "    \n",
    "    # Add red dot when there are too many points on the same side of the mean\n",
    "    axs[0].plot(data[data['Reason']==3]['Dim'], linestyle=\"\", marker='o', color='orange')    \n",
    "    \n",
    "    # Add red dot where Dim is over UCL or under LCL\n",
    "    axs[0].plot(data[data['Reason']==1]['Dim'], linestyle=\"\", marker='o', color='red')    \n",
    "\n",
    "    # Plot blue horizontal line at the process mean\n",
    "    axs[0].axhline(x_bar, color='blue')\n",
    "\n",
    "    # Plot red dotted lines at UCL and LCL\n",
    "    axs[0].axhline(ucl, color = 'red', linestyle = 'dashed')\n",
    "    axs[0].axhline(lcl, color = 'red', linestyle = 'dashed')\n",
    "\n",
    "    # Set Chart title and axis labels\n",
    "    axs[0].set_title('Individual Chart')\n",
    "    axs[0].set(xlabel='Part', ylabel='Measurement')\n",
    "\n",
    "\n",
    "    # mR chart\n",
    "    # Graph all the points \n",
    "    axs[1].plot( np.abs(data['variability']), linestyle='-', marker='o', color='black')\n",
    "\n",
    "    # Add red dot where Dim is over UCL\n",
    "    axs[1].plot(data[data['Reason']==2]['variability'], linestyle=\"\", marker='o', color='red')\n",
    "\n",
    "    # Plot blue horizontal line at the mR mean\n",
    "    axs[1].axhline(mr_bar, color='blue')\n",
    "\n",
    "    # Plot red dotted line at UCL\n",
    "    axs[1].axhline(mr_ucl, color='red', linestyle ='dashed')\n",
    "\n",
    "    axs[1].set_ylim(bottom=0)\n",
    "    axs[1].set_title('Moving Range Chart')\n",
    "    axs[1].set(xlabel='Part', ylabel='Range')\n",
    "    \n",
    "    return data[data['Reason']>0]\n",
    "    #return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b8c62-22a2-43bb-9545-ae927e9785ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"par3\"></a>\n",
    "## 3. Simulate the data using ```numpy.random``` package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28dfc21-5ec9-4b05-ba23-ccf7437892c0",
   "metadata": {},
   "source": [
    "<a id=\"par3.1\"></a>\n",
    "### 3.1 Simulate single variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda4b4fa-026f-4f20-908e-9d326b4b1128",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"par3.1.1\"></a>\n",
    "#### 3.1.1 Process with no special cause variability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569214b-e817-4bdb-bf6f-c8fbfe0f0f96",
   "metadata": {},
   "source": [
    "##### Define Dim1 attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc4f90-7190-4665-b632-2645ff86b447",
   "metadata": {},
   "source": [
    "Process capability:\n",
    "\n",
    "$$\n",
    "C_{p}=\\frac{USL-LSL}{6\\sigma}\n",
    "$$\n",
    "\n",
    "Given the process capability and upper and lower service limits, we can calculate the underlying process standard deviation:\n",
    "\n",
    "\n",
    "$$\n",
    "\\sigma=\\frac{USL-LSL}{6 C_{p}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe769c55-1168-4f8b-81c9-55d8d06e4020",
   "metadata": {},
   "source": [
    "##### Define process parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e47694-0c84-4916-b302-91d9521f78ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nominal dimension\n",
    "mean = 9.5\n",
    "# Upper and Lower tollerances\n",
    "lsl, usl = 9.4, 9.6\n",
    "# Process capability\n",
    "cp = 2.\n",
    "# Number of lots\n",
    "nr_lots = 20\n",
    "# Parts in the lot \n",
    "nr_in_lot = 10\n",
    "# Total number of parts\n",
    "n = nr_lots*nr_in_lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0e55b0-4cb7-49c8-bd9f-f71d6beea166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the measurements of single dimenension using normal distribution\n",
    "# ccv stands for common cause variation\n",
    "dim1 = pd.DataFrame({'ccv': rng.normal(mean, (usl-lsl)/(6*cp) , n)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f399dc-da1d-49cc-a76c-72b9dc2c46c8",
   "metadata": {},
   "source": [
    "##### Show distribution of dim1 using Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac04224f-7798-42c6-8172-f4c12cc08e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(dim1, 'Dim1', lsl, usl, cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ee5d9c-606e-4521-bf74-70766254c60b",
   "metadata": {},
   "source": [
    "##### Check Dim1 with Statistical Process Control charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7e208-8074-4609-841c-ac8141ac0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = xmr(dim1.loc[:,'ccv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f592f8c-f0f4-4885-86fc-9de6e0e641fb",
   "metadata": {},
   "source": [
    "##### Process not centered at nominal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba632da3-9561-435a-8aac-789e321d32b0",
   "metadata": {},
   "source": [
    "More realistic scenarion would be a situation where the manufacturing process is not perfectly centered on the nominal value. This can be caused by:\n",
    "- Lack of machine calibration\n",
    "- Poor process design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4999cd8f-9147-4795-8eb5-dc2b17747032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the effect of proces not being centered at the nominal\n",
    "# The mean of such process would be pooled from normal distribution:\n",
    "\n",
    "proc_mean=rng.normal(mean, (usl-lsl)/6)\n",
    "\n",
    "dim1['ccv'] = rng.normal(proc_mean, (usl-lsl)/(6*cp) , n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36c0b1b-e033-4a2a-9b64-145d9c5078d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As seen on the histogram plot, process without special causes variability is alredy not centered perfectly between upper and lower service limits\n",
    "hist(dim1['ccv'], 'Dim1', lsl, usl, cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e8c361-bd67-45e1-b5fd-3c9517679ea1",
   "metadata": {},
   "source": [
    "<a id=\"par3.1.2\"></a>\n",
    "#### 3.1.2 Simulate 'special causes'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e27e729-f386-42d3-b07c-baede70c4791",
   "metadata": {},
   "source": [
    "##### Variation between the batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9548554-a030-46c7-badc-33dc9c38d73b",
   "metadata": {},
   "source": [
    "Vatiation between the batches can be casued by:\n",
    "- difrences in the fixture setup\n",
    "- changes in the raw material (material hardness, forging size etc)\n",
    "\n",
    "Distribution of batch-to-batch variation is normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502aa7a1-2d46-4de3-bcc3-00acfcddf243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the ratio of batch-to-batch standard deviation to the dimension tolerance\n",
    "# The larger the number, the smallest the effect of this special cause \n",
    "btb_sd_ratio = 20\n",
    "\n",
    "# create a array with adjustment for each lot\n",
    "lots = rng.normal(0, (usl-lsl)/btb_sd_ratio , nr_lots)\n",
    "\n",
    "# add the same adjustment for each 'nr_in_lot' in a lot\n",
    "part = 0\n",
    "for l in lots:\n",
    "    for p in range(nr_in_lot):\n",
    "        dim1.loc[part, 'btb'] = l\n",
    "        part = part + 1\n",
    "\n",
    "\n",
    "# Create a 'measured' column by adding a batch-to-batch variation to the base ccv dimensions\n",
    "dim1['measured'] = dim1['ccv'] + dim1['btb']\n",
    "\n",
    "# Show data for parts in 2 first lots to make sure btb is calculated correctly       \n",
    "dim1.head(2*nr_in_lot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05da92c4-24bb-46c1-816c-50ac66e72aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the SPC chart for the measured dimension (common cause variation with batch-to-batch variation)\n",
    "o = xmr(dim1.loc[:,'measured'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f401a46d-9fec-4819-b47d-3462647ed6f3",
   "metadata": {},
   "source": [
    "##### Effect of the tool size variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb8c5f9-f5e8-4c20-beda-42a458c52286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tool life: how many lots can be processed by one tool\n",
    "lots_per_tool = 4\n",
    "# tool tolerance\n",
    "# Example tool tolerances: http://www.mitsubishicarbide.com/en/technical_information/tec_rotating_tools/face_mills/tec_milling_guide/tec_milling_tolerance\n",
    "tool_tol = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439b5603-febf-4267-91ab-b3b9a02a751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of changes in tool sizes will be pooled from uniform distribution from interval -tool_tol to 0\n",
    "nr_of_tools = int(np.ceil(nr_lots/lots_per_tool))\n",
    "tools = rng.uniform(-tool_tol, 0 , nr_of_tools)\n",
    "\n",
    "# add the same adjustment to each part processed by one tool: lots_per_tool * nr_in_lot - lots_per_tool is in number of lots\n",
    "part = 0\n",
    "for t in tools:\n",
    "    for p in range(nr_in_lot * lots_per_tool):\n",
    "        dim1.loc[part, 'tcv'] = t\n",
    "        part = part + 1\n",
    "        # when the ratio nr_lots/lots_per_tool is not an integer, additional rows would be created by this loop\n",
    "        # exiting the loop when part count reaches nr_lots*nr_in_lot\n",
    "        if part==nr_lots*nr_in_lot:\n",
    "            break\n",
    "\n",
    "# Sum all the variations:\n",
    "dim1['measured'] = dim1['ccv'] + dim1['btb'] + dim1['tcv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ace82-91be-49d9-97e3-6357c3e6e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the impact from batch-to-batch variation with tool-change variation\n",
    "dim1[['btb','tcv']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1146c177-a7d8-4c1c-91a8-29c1e55ecbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the SPC chart for the measured dimension (common cause variation plus batch-to-batch variation and tool-change variation)\n",
    "o = xmr(dim1.loc[:,'measured'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f20c9-06dc-4452-a4a0-0af72097e314",
   "metadata": {},
   "source": [
    "##### Effect of a tool wear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16fb05-a16f-47da-a4cf-9ed7b82b7a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool wear can be aproximated with a linear model, as per:\n",
    "# https://www.sciencedirect.com/science/article/pii/S235197891630049X/pdf?md5=de5447dd9c5745bc8a5638dc26d7d66c&pid=1-s2.0-S235197891630049X-main.pdf\n",
    "# https://journals.sagepub.com/doi/pdf/10.1177/1687814017750434\n",
    "\n",
    "# Define the maximum tool wear\n",
    "max_tool_wear = 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a45c25-ee31-4ae9-a671-97b92239e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tool wear assuming linear tool-wear model\n",
    "inc_wear = max_tool_wear/(nr_in_lot * lots_per_tool)\n",
    "\n",
    "part = 0\n",
    "\n",
    "for t in tools:\n",
    "    # No tool wear for the firts part\n",
    "    dim1.loc[part, 't_wear'] = 0\n",
    "    for p in range(1, nr_in_lot * lots_per_tool):\n",
    "        part = part + 1\n",
    "        # pool a random number between 0 and inc_wear and add it to the previous part wear to calculate the total tool wear for current part\n",
    "        dim1.loc[part, 't_wear'] =  dim1.loc[part-1, 't_wear'] + rng.uniform(0, inc_wear)\n",
    "        # when the ratio nr_lots/lots_per_tool is not an integer, additional rows would be created by this loop\n",
    "        # exiting the loop when part count reaches nr_lots*nr_in_lot\n",
    "        if part==nr_lots*nr_in_lot:\n",
    "            break\n",
    "\n",
    "dim1['measured'] = dim1['ccv'] + dim1['btb'] + dim1['tcv'] + dim1['t_wear']           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c44cc-3b66-4db0-92b2-8aaa09644200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the impact from batch-to-batch variation, tool-change variation and tool wear\n",
    "dim1[['btb','tcv', 't_wear']].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77716bc9-c069-4a72-8b59-87dbacf8c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the SPC chart for the measured dimension (common cause variation plus batch-to-batch variation and tool-change variation)\n",
    "o = xmr(dim1.loc[:,'measured'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9264bc1e-6edc-4176-a6e9-e778275a7fa4",
   "metadata": {},
   "source": [
    "##### Model tool failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee041ed-b131-477d-8796-0190d8de307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model number of parts machined by each tool using Weibull distribution\n",
    "# https://www.researchgate.net/publication/226636244_Prediction_of_tool_failure_rate_in_turning_hardened_steels\n",
    "\n",
    "# Shape parameter\n",
    "a = 12\n",
    "# scale parameter\n",
    "lam = nr_in_lot * lots_per_tool  \n",
    "# location parameter\n",
    "mu = nr_in_lot * lots_per_tool * 0.1\n",
    "\n",
    "# Round down to the nearest integer\n",
    "tool_life = np.floor(weibull_min.rvs(a, loc=mu, scale=lam, size=nr_of_tools))\n",
    "\n",
    "# define the x axis of the plot\n",
    "x = np.arange(1, 1000.)/1000.*lam\n",
    "\n",
    "y = weibull_min.pdf(x, a, loc=mu, scale=lam)\n",
    "plt.plot(x, y, color='green', label='Scaled cdf')\n",
    "plt.title('Probability of tool failure before the tool change: {:.2%}'.format(weibull_min.cdf(40, a, loc=mu, scale=lam)))\n",
    "plt.axvline(nr_in_lot * lots_per_tool, color='r', linestyle='dashed', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3257b-769f-4dcd-9a77-7995fa80ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7bc1b7-c97b-4985-a767-b7b6ce2c2934",
   "metadata": {},
   "outputs": [],
   "source": [
    "part = 0\n",
    "\n",
    "dim1['t_break'] = 0\n",
    "\n",
    "for tl in tool_life:\n",
    "    # if tool life is greater than number of parts processed by tool:\n",
    "    if tl>=nr_in_lot * lots_per_tool:\n",
    "        part = part + nr_in_lot * lots_per_tool\n",
    "        if part>=nr_lots*nr_in_lot:\n",
    "            break        \n",
    "    # if tool life is smaller than number of parts processed by tool:\n",
    "    else:\n",
    "        part = part + int(tl)\n",
    "        if part>=nr_lots*nr_in_lot:\n",
    "            break        \n",
    "        for p in range(int(tl), nr_in_lot * lots_per_tool):\n",
    "            # Assign random value from normal distribution centered around 0.1\n",
    "            # Chipped cutting tool will have smaller size causing large shift in the machined size\n",
    "            # but rougher edge will introduce more variation to the process\n",
    "            dim1.loc[part, 't_break'] = rng.normal(0.1, 0.01)\n",
    "            part = part + 1\n",
    "            # when the ratio nr_lots/lots_per_tool is not an integer, additional rows would be created by this loop\n",
    "            # exiting the loop when part count reaches nr_lots*nr_in_lot\n",
    "            if part>=nr_lots*nr_in_lot:\n",
    "                break\n",
    "                \n",
    "dim1['measured'] = dim1['ccv'] + dim1['btb'] + dim1['tcv'] + dim1['t_wear'] + dim1['t_break']     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65318442-2745-46e1-8e92-2d4c8ffb9b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the impact from batch-to-batch variation, tool-change variation, tool wear and tool failure\n",
    "dim1[['btb','tcv', 't_wear', 't_break']].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0c893-8c6f-4460-b005-b900a715f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the SPC chart for the measured dimension\n",
    "o = xmr(dim1.loc[:,'measured'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7afa2b-4317-45a3-9728-162172f5f75a",
   "metadata": {},
   "source": [
    "<a id=\"par3.1\"></a>\n",
    "### 3.2 Simulate 10 independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baf2f68-a4f0-4292-8eff-1504810262d1",
   "metadata": {},
   "source": [
    "##### Define process step parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f917b017-f936-47a6-b914-275018fb4697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of days\n",
    "n_days = 30\n",
    "# Numbers of batches (lots) manufactured in one day\n",
    "nr_lots = 20\n",
    "# Parts in a lot \n",
    "parts_in_lot = 10\n",
    "# Total number of parts\n",
    "n_parts = n_days* nr_lots * parts_in_lot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a8ce03-3d7c-48e0-9bdd-a519249c9c06",
   "metadata": {},
   "source": [
    "#### 3.2.1 Create a table of process definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04baeb2-7f8d-49e5-8cfb-5ad224991f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table of 10 process definitions:\n",
    "# Number of independent variables to be simulated:\n",
    "n_dim = 10\n",
    "\n",
    "# list of typical tolerances used in manufacturing process and their typical frequency of use\n",
    "tol_array = [0.02, 0.05, 0.10, 0.20]\n",
    "tol_freq =  [0.10, 0.20, 0.50, 0.20]\n",
    "\n",
    "dim_def = pd.DataFrame({'Nominal': np.round(rng.uniform(3, 10, n_dim), 1) })\n",
    "dim_def['tolerance'] = rng.choice(tol_array, n_dim, p=tol_freq )\n",
    "dim_def['LSL'] = dim_def['Nominal']-dim_def['tolerance']\n",
    "dim_def['USL'] = dim_def['Nominal']+dim_def['tolerance']\n",
    "\n",
    "dim_def['Cp'] = rng.normal(2, 0.5, n_dim)\n",
    "\n",
    "# number of parts made by single tool will be pooled from natural numbers between 1 and 9:\n",
    "dim_def['lots_per_tool'] = rng.choice(range(1,10), n_dim)\n",
    "\n",
    "# tool lolerance will be randomly selected from th efollowing list:\n",
    "dim_def['tool_tol '] = rng.choice([0.01, 0.02, 0.04], n_dim)\n",
    "\n",
    "# max tool wear per tool\n",
    "dim_def['max_tool_wear'] = rng.choice([0.01, 0.02, 0.04], n_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tool failure probability from Webul distribution\n",
    "a = 12\n",
    "dim_def['tool_weibul_a'] = a\n",
    "dim_def['tool_weibul_loc'] = parts_in_lot * dim_def['lots_per_tool'] * rng.uniform(0.1, 0.3, n_dim)\n",
    "dim_def['tool_weibul_scale'] = parts_in_lot * dim_def['lots_per_tool']\n",
    "dim_def['tool_failure_prob'] = weibull_min.cdf(parts_in_lot * dim_def['lots_per_tool'], a, loc=dim_def['tool_weibul_loc'], scale=dim_def['tool_weibul_scale'])\n",
    "\n",
    "dim_def"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454e11c5-e759-4458-bb55-aabedecf1a6f",
   "metadata": {},
   "source": [
    "####  3.2.2 Create a table with dimension measurements for all the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb37f5d7-e7b4-4c45-acee-5a68565f4b02",
   "metadata": {},
   "source": [
    "##### Prepare DataFrame to hold all the measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2952c55-2518-4cab-b001-fab3da9dd6ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an all_dims Dataframe with a first columns: 'Date' and 'Lot'\n",
    "\n",
    "days = []\n",
    "lots = []\n",
    "\n",
    "for day in range(n_days):\n",
    "    for lot in range(nr_lots):        \n",
    "        for part in range(parts_in_lot):\n",
    "            # There will be 'parts_in_lot * nr_lots' parts made in every day, all these rows will have the same value in the 'Date' column\n",
    "            days.append(day)\n",
    "            # Make sure that lot number is unique for every batch\n",
    "            lots.append(day*nr_lots + lot)\n",
    "        \n",
    "all_dims = pd.DataFrame({'Date': days, 'Lot': lots})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937eac2-06b2-4da7-bff9-caed34ec2466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the n_dim number of features processed over n_days days:\n",
    "\n",
    "all_parts = nr_lots*parts_in_lot*n_days\n",
    "\n",
    "# Iterate through the columns of all_dims DataFrame:\n",
    "for row, index in dim_def.iterrows():\n",
    "    column = \"Dim\" + str(row + 1)\n",
    "    mean = index[0]\n",
    "    tol = index[1] * 2\n",
    "    cp = index[4]\n",
    "    # Tool parameters:\n",
    "    lots_per_tool = int(index[5])\n",
    "    tool_tol = index[6]\n",
    "    max_tool_wear = index[7]\n",
    "    # Tool failure Weibull distribution parameters\n",
    "    # Shape parameter\n",
    "    a = index[8]\n",
    "    # scale parameter\n",
    "    mu = index[9]\n",
    "    # location parameter\n",
    "    lam = index[10]\n",
    "    \n",
    "    # Simulate the effect of proces not being centered at the nominal\n",
    "    # The mean of such process would be pooled from normal distribution:\n",
    "    proc_mean=rng.normal(mean, tol/6)\n",
    "    \n",
    "    dim = pd.DataFrame({'ccv': rng.normal(proc_mean, tol/(6*cp) , n_parts)})\n",
    "    \n",
    "    # ADD BATCH TO BATCH VARIATION\n",
    "    \n",
    "    # define the ratio of batch-to-batch standard deviation to the dimension tolerance\n",
    "    # The larger the number, the smallest the effect of this special cause \n",
    "    btb_sd_ratio = rng.normal(20, 5)\n",
    "\n",
    "    # create an array with adjustment for mean for each lot\n",
    "    lots = rng.normal(0, tol/btb_sd_ratio , nr_lots * n_days)\n",
    "\n",
    "    # add the same adjustment for each 'parts_in_lot' in a lot\n",
    "    part = 0\n",
    "    for l in lots:\n",
    "        for p in range(parts_in_lot):\n",
    "            dim.loc[part, 'btb'] = l\n",
    "            part = part + 1\n",
    "\n",
    "            \n",
    "    #ADD TOOL CHANGE VARIATION and EFFECT OF A TOOL WEAR\n",
    "    \n",
    "    # Calculate the tool wear assuming linear tool-wear model\n",
    "    inc_wear = max_tool_wear/(nr_in_lot * lots_per_tool)\n",
    "    \n",
    "    # Distribution of changes in tool sizes will be pooled from uniform distribution from interval -tool_tol to 0\n",
    "    nr_of_tools = int(np.ceil(nr_lots * n_days/lots_per_tool))\n",
    "    tools = rng.uniform(-tool_tol, 0 , nr_of_tools)\n",
    "\n",
    "    \n",
    "    part = 0\n",
    "    for t in tools:\n",
    "        # No tool wear for the firts part processed by given tool\n",
    "        dim.loc[part, 't_wear'] = 0\n",
    "        for p in range(parts_in_lot * lots_per_tool):\n",
    "            dim.loc[part, 'tcv'] = t\n",
    "            part = part + 1\n",
    "            dim.loc[part, 't_wear'] =  dim.loc[part-1, 't_wear'] + rng.uniform(0, inc_wear)\n",
    "            # when the ratio nr_lots/lots_per_tool is not an integer, additional rows would be created by this loop\n",
    "            # exiting the loop when part count reachesall_parts = nr_lots*parts_in_lot*n_days\n",
    "            if part==all_parts:\n",
    "                break    \n",
    "    \n",
    "    \n",
    "    # ADD EFFECT OF EARLY TOOL FAILURE\n",
    "\n",
    "\n",
    "    # tool_life: number of processed part before tool failure, rounded down to the nearest integer:\n",
    "    tool_life = np.floor(weibull_min.rvs(a, loc=mu, scale=lam, size=nr_of_tools))\n",
    "    # initialize 't_break' column with 0s\n",
    "    dim['t_break'] = 0\n",
    "\n",
    "    for tl in tool_life:\n",
    "        # if tool life is greater than number of parts processed by tool:\n",
    "        if tl>=nr_in_lot * lots_per_tool:\n",
    "            part = part + nr_in_lot * lots_per_tool\n",
    "            if part>=nr_lots*nr_in_lot:\n",
    "                break        \n",
    "        # if tool life is smaller than number of parts processed by tool:\n",
    "        else:\n",
    "            part = part + int(tl)\n",
    "            if part>=nr_lots*nr_in_lot:\n",
    "                break        \n",
    "            for p in range(int(tl), nr_in_lot * lots_per_tool):\n",
    "                # Assign random value from normal distribution centered around 0.1\n",
    "                # Chipped cutting tool will have smaller size causing large shift in the machined size\n",
    "                # but rougher edge will introduce more variation to the process\n",
    "                dim.loc[part, 't_break'] = rng.normal(0.1, 0.01)\n",
    "                part = part + 1\n",
    "                # when the ratio nr_lots/lots_per_tool is not an integer, additional rows would be created by this loop\n",
    "                # exiting the loop when part count reaches nr_lots*nr_in_lot\n",
    "                if part>=nr_lots*nr_in_lot:\n",
    "                    break\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ADD EFFECT OF COMMON CAUSES VARIATION AND ALL SPECIAL CAUSES \n",
    "\n",
    "    # Create a 'measured' column by adding a batch-to-batch variation to the base ccv dimensions\n",
    "    all_dims[column] = dim['ccv'] + dim['btb'] + dim['tcv'] + dim['t_wear'] + dim['t_break']\n",
    "    \n",
    "    \n",
    "    \n",
    "all_dims    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766666ce-71bb-4e42-907e-283b4ebcfc9c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if any of the 10 dimensions in each part is outside of tolerance band\n",
    "# Mark part as scrap if at least one dimension is outside of tolerance\n",
    "\n",
    "all_dims['Scrap']=0\n",
    "n_dim = len(dim_def.index)\n",
    "\n",
    "for part_row, part_index in all_dims.iterrows():\n",
    "    \n",
    "    dim_index = 0\n",
    "    for col in part_index[2:(2+n_dim)]:\n",
    "        scrap = False\n",
    "        if dim_def.iloc[dim_index,2] > col or col > dim_def.iloc[dim_index,3]:\n",
    "            scrap = True\n",
    "            #print(dim_def.iloc[dim_index,2], col, dim_def.iloc[dim_index,3])\n",
    "        dim_index += 1\n",
    "        if scrap:\n",
    "            all_dims.loc[part_row, 'Scrap'] = 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356f451-acab-445c-9bb5-98e4e29b3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dims.groupby('Date').sum()['Scrap'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caba1b7-41d7-405f-994b-91d6090ee1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all the dimensions distribution on histogram with LSL and USL marked\n",
    "for row, index in dim_def.iterrows():\n",
    "    column = \"Dim\" + str(row + 1)\n",
    "    hist(all_dims[column], column, dim_def.loc[row,'LSL'], dim_def.loc[row,'USL'], dim_def.loc[row,'Cp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e2ac24-a24b-4b85-b11e-0649786ec1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all the dimensions distribution on histogram with LSL and USL marked\n",
    "for row, index in dim_def.iterrows():\n",
    "    column = \"Dim\" + str(row + 1)\n",
    "    xBarS(all_dims, column, 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863bfbd-93b2-461f-97d9-945b5e2ae22f",
   "metadata": {},
   "source": [
    "<a id=\"par4\"></a>\n",
    "## 4. Analyse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3cb3a4-974b-4f4f-8786-f0fee1e3bdf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2607a963-31be-4aa7-899e-3a650d37cb19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395ade3-3c5b-4582-bce1-93671dcae204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b74639f6-65b1-46e4-b307-1243dd039636",
   "metadata": {},
   "source": [
    "## 5. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c6dc4c-4da5-4a27-a5de-2f32189b6dfa",
   "metadata": {},
   "source": [
    "[[1] VARIABILITY IN MANUFACTURING](https://link.springer.com/referenceworkentry/10.1007%2F1-4020-0612-8_1032#Chap1031_1-4020-0612-8_1031)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b718478-c8cd-4f1e-a055-ff0e152b6373",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
